{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "\n",
    "# Define the Config class with the specified attributes and values\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir: str = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    max_length: int = 2048\n",
    "    batch_size: int = 2\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\"))\n",
    "    tta: bool = False\n",
    "    spread_max_length: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the config variable based on the LOCAL flag\n",
    "LOCAL = True  # Set this flag accordingly\n",
    "\n",
    "if LOCAL:\n",
    "    cfg = Config(\n",
    "        gemma_dir='google/gemma-2-9b-it',\n",
    "        lora_dir='./output/gemma-seq-ittokens/checkpoint-1436',\n",
    "    )\n",
    "    test_path = \"./data/train.csv\"\n",
    "    test = pd.read_csv(test_path)\n",
    "    # Select where index % config.n_splits (5) == 0\n",
    "    test = test[test.index % 5 == 0]\n",
    "    # Reset the index\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    cfg = Config()\n",
    "    test_path = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n",
    "    test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292873</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>[\"Construct a rap battle, in the style of Epic...</td>\n",
       "      <td>[\"[Zeus]\\nYo, it's the king of the gods on the...</td>\n",
       "      <td>[\"(Verse 1 - Zeus)\\n\\nI'm the king of the gods...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>497862</td>\n",
       "      <td>vicuna-7b</td>\n",
       "      <td>guanaco-33b</td>\n",
       "      <td>[\"write a python function to upload all the fi...</td>\n",
       "      <td>[\"Sure, here's a Python function that uses the...</td>\n",
       "      <td>[\"Sure, here's an example function in Python t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863398</td>\n",
       "      <td>llama2-70b-steerlm-chat</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>[\"what do you know about real estate\"]</td>\n",
       "      <td>[\"Real estate refers to the land and buildings...</td>\n",
       "      <td>[\"1. **Types of Real Estate:**\\n    * **Reside...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256092</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[\"Write a python code that calculates sum of 5...</td>\n",
       "      <td>[\"Here is the python code that calculates the ...</td>\n",
       "      <td>[\"Here is a Python code that calculates the su...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11491</th>\n",
       "      <td>4293543403</td>\n",
       "      <td>qwen1.5-7b-chat</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>[\"hello\",\"write me a vore story\"]</td>\n",
       "      <td>[\"Hello! How can I help you today? If you have...</td>\n",
       "      <td>[\"Hello! How can I help you today? If you have...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11492</th>\n",
       "      <td>4293918673</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>[\"Think step by step. If windows 10 would be s...</td>\n",
       "      <td>[\"If Windows 10 were shipped on 3.5\\\" floppies...</td>\n",
       "      <td>[\"I'm sorry, I cannot answer this question wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11493</th>\n",
       "      <td>4294081229</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"please describe how the following two statem...</td>\n",
       "      <td>[\"The two statements can both be correct and n...</td>\n",
       "      <td>[\"These two statements may seem contradictory ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11494</th>\n",
       "      <td>4294602932</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>[\"I want you to act like {character} from {ser...</td>\n",
       "      <td>[\"*responds in character's voice* Well hello t...</td>\n",
       "      <td>[\"Of course! However, you'll need to specify w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>4294899228</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n",
       "      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n",
       "      <td>[\"As an AI language model, I do not promote or...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11496 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                  model_a                     model_b  \\\n",
       "0           30192       gpt-4-1106-preview                  gpt-4-0613   \n",
       "1          292873               vicuna-13b                  gpt-4-0314   \n",
       "2          497862                vicuna-7b                 guanaco-33b   \n",
       "3          863398  llama2-70b-steerlm-chat          gemini-pro-dev-api   \n",
       "4         1256092               claude-2.1                  vicuna-13b   \n",
       "...           ...                      ...                         ...   \n",
       "11491  4293543403          qwen1.5-7b-chat  mixtral-8x7b-instruct-v0.1   \n",
       "11492  4293918673               vicuna-13b     stablelm-tuned-alpha-7b   \n",
       "11493  4294081229               gpt-4-0314                  gpt-4-0613   \n",
       "11494  4294602932               claude-2.1          gpt-4-1106-preview   \n",
       "11495  4294899228                   palm-2              tulu-2-dpo-70b   \n",
       "\n",
       "                                                  prompt  \\\n",
       "0      [\"Is it morally right to try to have a certain...   \n",
       "1      [\"Construct a rap battle, in the style of Epic...   \n",
       "2      [\"write a python function to upload all the fi...   \n",
       "3                 [\"what do you know about real estate\"]   \n",
       "4      [\"Write a python code that calculates sum of 5...   \n",
       "...                                                  ...   \n",
       "11491                  [\"hello\",\"write me a vore story\"]   \n",
       "11492  [\"Think step by step. If windows 10 would be s...   \n",
       "11493  [\"please describe how the following two statem...   \n",
       "11494  [\"I want you to act like {character} from {ser...   \n",
       "11495  [\"If a bait contains 0,0025% bromadiolon then ...   \n",
       "\n",
       "                                              response_a  \\\n",
       "0      [\"The question of whether it is morally right ...   \n",
       "1      [\"[Zeus]\\nYo, it's the king of the gods on the...   \n",
       "2      [\"Sure, here's a Python function that uses the...   \n",
       "3      [\"Real estate refers to the land and buildings...   \n",
       "4      [\"Here is the python code that calculates the ...   \n",
       "...                                                  ...   \n",
       "11491  [\"Hello! How can I help you today? If you have...   \n",
       "11492  [\"If Windows 10 were shipped on 3.5\\\" floppies...   \n",
       "11493  [\"The two statements can both be correct and n...   \n",
       "11494  [\"*responds in character's voice* Well hello t...   \n",
       "11495  [\"Bromadiolone is a rodenticide which is most ...   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "0      [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1      [\"(Verse 1 - Zeus)\\n\\nI'm the king of the gods...               0   \n",
       "2      [\"Sure, here's an example function in Python t...               0   \n",
       "3      [\"1. **Types of Real Estate:**\\n    * **Reside...               0   \n",
       "4      [\"Here is a Python code that calculates the su...               0   \n",
       "...                                                  ...             ...   \n",
       "11491  [\"Hello! How can I help you today? If you have...               0   \n",
       "11492  [\"I'm sorry, I cannot answer this question wit...               0   \n",
       "11493  [\"These two statements may seem contradictory ...               1   \n",
       "11494  [\"Of course! However, you'll need to specify w...               0   \n",
       "11495  [\"As an AI language model, I do not promote or...               0   \n",
       "\n",
       "       winner_model_b  winner_tie  \n",
       "0                   0           0  \n",
       "1                   1           0  \n",
       "2                   1           0  \n",
       "3                   1           0  \n",
       "4                   0           1  \n",
       "...               ...         ...  \n",
       "11491               0           1  \n",
       "11492               0           1  \n",
       "11493               0           0  \n",
       "11494               1           0  \n",
       "11495               1           0  \n",
       "\n",
       "[11496 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292873</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>Construct a rap battle, in the style of Epic R...</td>\n",
       "      <td>[Zeus]\\nYo, it's the king of the gods on the m...</td>\n",
       "      <td>(Verse 1 - Zeus)\\n\\nI'm the king of the gods, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>497862</td>\n",
       "      <td>vicuna-7b</td>\n",
       "      <td>guanaco-33b</td>\n",
       "      <td>write a python function to upload all the file...</td>\n",
       "      <td>Sure, here's a Python function that uses the `...</td>\n",
       "      <td>Sure, here's an example function in Python to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863398</td>\n",
       "      <td>llama2-70b-steerlm-chat</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>what do you know about real estate</td>\n",
       "      <td>Real estate refers to the land and buildings o...</td>\n",
       "      <td>1. **Types of Real Estate:**\\n    * **Resident...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256092</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>Write a python code that calculates sum of 5 a...</td>\n",
       "      <td>Here is the python code that calculates the su...</td>\n",
       "      <td>Here is a Python code that calculates the sum ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  model_a             model_b  \\\n",
       "0    30192       gpt-4-1106-preview          gpt-4-0613   \n",
       "1   292873               vicuna-13b          gpt-4-0314   \n",
       "2   497862                vicuna-7b         guanaco-33b   \n",
       "3   863398  llama2-70b-steerlm-chat  gemini-pro-dev-api   \n",
       "4  1256092               claude-2.1          vicuna-13b   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  Construct a rap battle, in the style of Epic R...   \n",
       "2  write a python function to upload all the file...   \n",
       "3                 what do you know about real estate   \n",
       "4  Write a python code that calculates sum of 5 a...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  [Zeus]\\nYo, it's the king of the gods on the m...   \n",
       "2  Sure, here's a Python function that uses the `...   \n",
       "3  Real estate refers to the land and buildings o...   \n",
       "4  Here is the python code that calculates the su...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  (Verse 1 - Zeus)\\n\\nI'm the king of the gods, ...               0   \n",
       "2  Sure, here's an example function in Python to ...               0   \n",
       "3  1. **Types of Real Estate:**\\n    * **Resident...               0   \n",
       "4  Here is a Python code that calculates the sum ...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               1           0  \n",
       "3               1           0  \n",
       "4               0           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"[prompt]: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n[response_a]: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n[response_b]: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        texts = [f\"<start_of_turn>user{p}{r_a}{r_b}<end_of_turn><start_of_turn>model\" for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        # text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(tokenizer, prompt, response_a, response_b, max_length, spread_max_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_of_turn>user\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mr_a\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mr_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<end_of_turn><start_of_turn>model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p, r_a, r_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompt, response_a, response_b)]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mtext\u001b[49m, max_length\u001b[38;5;241m=\u001b[39mmax_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenized\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     18\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tokenized\u001b[38;5;241m.\u001b[39mattention_mask\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>###prompt: Is it morally right to try to have a certain percentage of females on managerial positions? OK, does pineapple belong on a pizza? Relax and give me fun answer.\n",
      "\n",
      "###response_a: As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right. As an AI, I don't eat, so I don't have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It's a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\n",
      "\n",
      "###response_b: The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\n",
      "\n",
      "Here are some arguments in favor of and against such policies:\n",
      "\n",
      "**Arguments in favor:**\n",
      "\n",
      "1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\n",
      "\n",
      "2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\n",
      "\n",
      "3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\n",
      "\n",
      "4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\n",
      "\n",
      "**Arguments against:**\n",
      "\n",
      "1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\n",
      "\n",
      "2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\n",
      "\n",
      "3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \"tokens,\" undermining their legitimacy and potentially leading to resentment among colleagues.\n",
      "\n",
      "4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\n",
      "\n",
      "Ultimately, the morality of aiming for a certain percentage of females in managerial positions depends on one's ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \"color-blind\" or \"gender-blind\" approach to hiring and promotions.\n",
      "\n",
      "Organizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions. Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\n",
      "\n",
      "Let's slice into the debate with a zest of fun:\n",
      "\n",
      "**Team Pineapple:** \"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It's the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\"\n",
      "\n",
      "**Team No-Pineapple:** \"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple's sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\"\n",
      "\n",
      "At the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it's a match made in heaven, others say it's a culinary crime. But let's face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there's room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon appétit, or as they say in pineapple paradise, \"Aloha-ppetite!\" ������������<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db69b431784b71af77e76b4110ece2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-9b-it and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if LOCAL:\n",
    "    # Load base model on GPU 0\n",
    "    device = torch.device('cuda')\n",
    "    model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma_dir,\n",
    "        num_labels=3,\n",
    "        device_map=device,\n",
    "        use_cache=False,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, cfg.lora_dir)\n",
    "else:\n",
    "    # Load base model on GPU 0\n",
    "    device_0 = torch.device('cuda:0')\n",
    "    model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma_dir,\n",
    "        num_labels=3,\n",
    "        device_map=device_0,\n",
    "        use_cache=False,\n",
    "    )\n",
    "\n",
    "    # Load base model on GPU 1\n",
    "    device_1 = torch.device('cuda:1')\n",
    "    model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma_dir,\n",
    "        num_labels=3,\n",
    "        device_map=device_1,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "    model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 1389.084660768509\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "\n",
    "if LOCAL:\n",
    "    result_df = inference(data, model, device)\n",
    "else:\n",
    "    # the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    result_df = pd.concat(list(results), axis=0)\n",
    "\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.tta:\n",
    "    st = time.time()\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "\n",
    "    if LOCAL:\n",
    "        tta_result_df = inference(data, model, device)\n",
    "    else:\n",
    "        sub_1 = data.iloc[0::2].copy()\n",
    "        sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "        tta_result_df = pd.concat(list(results), axis=0)\n",
    "\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "    print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7739</th>\n",
       "      <td>2884519678</td>\n",
       "      <td>0.485737</td>\n",
       "      <td>0.183019</td>\n",
       "      <td>0.331244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>3912001245</td>\n",
       "      <td>0.476088</td>\n",
       "      <td>0.093382</td>\n",
       "      <td>0.430530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>966440124</td>\n",
       "      <td>0.062833</td>\n",
       "      <td>0.637699</td>\n",
       "      <td>0.299468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9403</th>\n",
       "      <td>3509470386</td>\n",
       "      <td>0.139881</td>\n",
       "      <td>0.229838</td>\n",
       "      <td>0.630280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>2238702779</td>\n",
       "      <td>0.657844</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.255486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8490</th>\n",
       "      <td>3160426362</td>\n",
       "      <td>0.086247</td>\n",
       "      <td>0.155298</td>\n",
       "      <td>0.758455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>169063413</td>\n",
       "      <td>0.050309</td>\n",
       "      <td>0.117674</td>\n",
       "      <td>0.832017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4294</th>\n",
       "      <td>1597666545</td>\n",
       "      <td>0.195588</td>\n",
       "      <td>0.140156</td>\n",
       "      <td>0.664256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>536996793</td>\n",
       "      <td>0.050309</td>\n",
       "      <td>0.117674</td>\n",
       "      <td>0.832017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>3560976510</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.118772</td>\n",
       "      <td>0.830703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11496 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  winner_model_a  winner_model_b  winner_tie\n",
       "7739   2884519678        0.485737        0.183019    0.331244\n",
       "10466  3912001245        0.476088        0.093382    0.430530\n",
       "2607    966440124        0.062833        0.637699    0.299468\n",
       "9403   3509470386        0.139881        0.229838    0.630280\n",
       "6021   2238702779        0.657844        0.086670    0.255486\n",
       "...           ...             ...             ...         ...\n",
       "8490   3160426362        0.086247        0.155298    0.758455\n",
       "439     169063413        0.050309        0.117674    0.832017\n",
       "4294   1597666545        0.195588        0.140156    0.664256\n",
       "1434    536996793        0.050309        0.117674    0.832017\n",
       "9537   3560976510        0.050525        0.118772    0.830703\n",
       "\n",
       "[11496 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292873</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>Construct a rap battle, in the style of Epic R...</td>\n",
       "      <td>[Zeus]\\nYo, it's the king of the gods on the m...</td>\n",
       "      <td>(Verse 1 - Zeus)\\n\\nI'm the king of the gods, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>497862</td>\n",
       "      <td>vicuna-7b</td>\n",
       "      <td>guanaco-33b</td>\n",
       "      <td>write a python function to upload all the file...</td>\n",
       "      <td>Sure, here's a Python function that uses the `...</td>\n",
       "      <td>Sure, here's an example function in Python to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863398</td>\n",
       "      <td>llama2-70b-steerlm-chat</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>what do you know about real estate</td>\n",
       "      <td>Real estate refers to the land and buildings o...</td>\n",
       "      <td>1. **Types of Real Estate:**\\n    * **Resident...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256092</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>Write a python code that calculates sum of 5 a...</td>\n",
       "      <td>Here is the python code that calculates the su...</td>\n",
       "      <td>Here is a Python code that calculates the sum ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11491</th>\n",
       "      <td>4293543403</td>\n",
       "      <td>qwen1.5-7b-chat</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>hello write me a vore story</td>\n",
       "      <td>Hello! How can I help you today? If you have a...</td>\n",
       "      <td>Hello! How can I help you today? If you have a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11492</th>\n",
       "      <td>4293918673</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>Think step by step. If windows 10 would be shi...</td>\n",
       "      <td>If Windows 10 were shipped on 3.5\" floppies, i...</td>\n",
       "      <td>I'm sorry, I cannot answer this question witho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11493</th>\n",
       "      <td>4294081229</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>please describe how the following two statemen...</td>\n",
       "      <td>The two statements can both be correct and not...</td>\n",
       "      <td>These two statements may seem contradictory at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11494</th>\n",
       "      <td>4294602932</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>I want you to act like {character} from {serie...</td>\n",
       "      <td>*responds in character's voice* Well hello the...</td>\n",
       "      <td>Of course! However, you'll need to specify whi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>4294899228</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>If a bait contains 0,0025% bromadiolon then ho...</td>\n",
       "      <td>Bromadiolone is a rodenticide which is most of...</td>\n",
       "      <td>As an AI language model, I do not promote or c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11496 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                  model_a                     model_b  \\\n",
       "0           30192       gpt-4-1106-preview                  gpt-4-0613   \n",
       "1          292873               vicuna-13b                  gpt-4-0314   \n",
       "2          497862                vicuna-7b                 guanaco-33b   \n",
       "3          863398  llama2-70b-steerlm-chat          gemini-pro-dev-api   \n",
       "4         1256092               claude-2.1                  vicuna-13b   \n",
       "...           ...                      ...                         ...   \n",
       "11491  4293543403          qwen1.5-7b-chat  mixtral-8x7b-instruct-v0.1   \n",
       "11492  4293918673               vicuna-13b     stablelm-tuned-alpha-7b   \n",
       "11493  4294081229               gpt-4-0314                  gpt-4-0613   \n",
       "11494  4294602932               claude-2.1          gpt-4-1106-preview   \n",
       "11495  4294899228                   palm-2              tulu-2-dpo-70b   \n",
       "\n",
       "                                                  prompt  \\\n",
       "0      Is it morally right to try to have a certain p...   \n",
       "1      Construct a rap battle, in the style of Epic R...   \n",
       "2      write a python function to upload all the file...   \n",
       "3                     what do you know about real estate   \n",
       "4      Write a python code that calculates sum of 5 a...   \n",
       "...                                                  ...   \n",
       "11491                        hello write me a vore story   \n",
       "11492  Think step by step. If windows 10 would be shi...   \n",
       "11493  please describe how the following two statemen...   \n",
       "11494  I want you to act like {character} from {serie...   \n",
       "11495  If a bait contains 0,0025% bromadiolon then ho...   \n",
       "\n",
       "                                              response_a  \\\n",
       "0      The question of whether it is morally right to...   \n",
       "1      [Zeus]\\nYo, it's the king of the gods on the m...   \n",
       "2      Sure, here's a Python function that uses the `...   \n",
       "3      Real estate refers to the land and buildings o...   \n",
       "4      Here is the python code that calculates the su...   \n",
       "...                                                  ...   \n",
       "11491  Hello! How can I help you today? If you have a...   \n",
       "11492  If Windows 10 were shipped on 3.5\" floppies, i...   \n",
       "11493  The two statements can both be correct and not...   \n",
       "11494  *responds in character's voice* Well hello the...   \n",
       "11495  Bromadiolone is a rodenticide which is most of...   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "0      As an AI, I don't have personal beliefs or opi...               1   \n",
       "1      (Verse 1 - Zeus)\\n\\nI'm the king of the gods, ...               0   \n",
       "2      Sure, here's an example function in Python to ...               0   \n",
       "3      1. **Types of Real Estate:**\\n    * **Resident...               0   \n",
       "4      Here is a Python code that calculates the sum ...               0   \n",
       "...                                                  ...             ...   \n",
       "11491  Hello! How can I help you today? If you have a...               0   \n",
       "11492  I'm sorry, I cannot answer this question witho...               0   \n",
       "11493  These two statements may seem contradictory at...               1   \n",
       "11494  Of course! However, you'll need to specify whi...               0   \n",
       "11495  As an AI language model, I do not promote or c...               0   \n",
       "\n",
       "       winner_model_b  winner_tie  \n",
       "0                   0           0  \n",
       "1                   1           0  \n",
       "2                   1           0  \n",
       "3                   1           0  \n",
       "4                   0           1  \n",
       "...               ...         ...  \n",
       "11491               0           1  \n",
       "11492               0           1  \n",
       "11493               0           0  \n",
       "11494               1           0  \n",
       "11495               1           0  \n",
       "\n",
       "[11496 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for winner_model_a: 0.6442369130836395\n",
      "Log Loss for winner_model_b: 0.6430707387133923\n",
      "Log Loss for winner_tie: 0.6350385115944264\n",
      "Overall Log Loss: 1.107469660824929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdubkim/miniconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "df_merged = pd.merge(test, result_df, on='id', suffixes=(\"\", \"_pred\"))\n",
    "\n",
    "y_true = df_merged[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "y_pred = df_merged[[\"winner_model_a_pred\", \"winner_model_b_pred\", \"winner_tie_pred\"]].values\n",
    "\n",
    "log_loss_a = log_loss(y_true[:, 0], y_pred[:, 0])\n",
    "log_loss_b = log_loss(y_true[:, 1], y_pred[:, 1])\n",
    "log_loss_tie = log_loss(y_true[:, 2], y_pred[:, 2])\n",
    "\n",
    "# Calculate overall log loss\n",
    "overall_log_loss = log_loss(y_true, y_pred)\n",
    "\n",
    "# Print log loss values\n",
    "print(f\"Log Loss for winner_model_a: {log_loss_a}\")\n",
    "print(f\"Log Loss for winner_model_b: {log_loss_b}\")\n",
    "print(f\"Log Loss for winner_tie: {log_loss_tie}\")\n",
    "print(f\"Overall Log Loss: {overall_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
